{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53011a2",
   "metadata": {},
   "source": [
    "this script expects that the tar file containing multiple json files of stories has been extracted into the \"data_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/50 files.\n",
      "Elapsed time: 16.98 seconds\n",
      "Estimated time remaining: 832.23 seconds\n",
      "Finished 2/50 files.\n",
      "Elapsed time: 36.84 seconds\n",
      "Estimated time remaining: 884.05 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tokenizers import Tokenizer\n",
    "import time\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")\n",
    "\n",
    "data_directory = '../data/TinyStories/all_data/'\n",
    "json_files = [file for file in os.listdir(data_directory) if file.endswith('.json')]\n",
    "\n",
    "out_directory = './tokenized_data/'\n",
    "\n",
    "files_finished = 0\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "tokens_total = 0\n",
    "stories_total = 0\n",
    "tokens_files = {}\n",
    "# loop over files\n",
    "for file in json_files:\n",
    "  tokens_file = 0\n",
    "  # open file\n",
    "  with open(os.path.join(data_directory, file), 'r') as f:\n",
    "    # load json data\n",
    "    data = json.load(f)\n",
    "\n",
    "  out_file = os.path.join(out_directory, file.replace('.json', '.txt'))\n",
    "  # open output file\n",
    "  with open(out_file, 'w') as out_f:\n",
    "    # loop over data\n",
    "    for i in range(len(data)):\n",
    "      # get story\n",
    "      story = data[i][\"story\"]\n",
    "      # get tokens\n",
    "      tokens = tokenizer.encode(story).ids\n",
    "      tokens_total += len(tokens)\n",
    "      tokens_file += len(tokens)\n",
    "      # write tokens to file\n",
    "      out_f.write(\" \".join(map(str, tokens)) + \"\\n\")\n",
    "\n",
    "  tokens_files[out_file] = tokens_file\n",
    "\n",
    "  \n",
    "  files_finished += 1\n",
    "  print(f\"Finished {files_finished}/{len(json_files)} files.\")\n",
    "\n",
    "  end_time = time.time()\n",
    "  elapsed_time = end_time - start_time\n",
    "  remaining_files = len(json_files) - files_finished\n",
    "  estimated_time = (elapsed_time / files_finished) * remaining_files\n",
    "  print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "  print(f\"Estimated time remaining: {estimated_time:.2f} seconds\")\n",
    "\n",
    "  if len(tokens_files) > 1:\n",
    "    break\n",
    "\n",
    "# save total tokens and tokens per file to file in out_directory\n",
    "with open(os.path.join(out_directory, 'meta.json'), 'w') as f:\n",
    "  json.dump({\n",
    "    'tokens_total': tokens_total,\n",
    "    'stories_total': stories_total,\n",
    "    'tokens_files': tokens_files\n",
    "  }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1022e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
