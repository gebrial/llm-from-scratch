{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c22a32f-cb2c-47cd-96fc-fea8953ac491",
   "metadata": {},
   "source": [
    "This script contains some improvement from train.ipynb by default:\n",
    "* much few layers, heads, and embedding dimension to reduce the model size\n",
    "* dataloader v2 which uses a custom tokenizer (again to reduce model size)\n",
    "* no positional embeddings (to reduce model complexity)\n",
    "* weight tying (to reduce model size)\n",
    "\n",
    "\n",
    "We implemented a few things here first and not before:\n",
    "* validation losses\n",
    "* increased the model size to be just below 30M parameters\n",
    "* reduced the amount of data trained with to keep the training (wall) time consistent\n",
    "* made graph more informative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce834d77-2417-42fc-b17f-d30e90267ea7",
   "metadata": {},
   "source": [
    "This script contains a couple improvements from train2.ipynb:\n",
    "* gradient accumulation is enabled\n",
    "* the dataloader chunks from the start of an example up to the max_length or the endoftext token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cab6bf-b4da-4f70-8c24-b249c7645936",
   "metadata": {},
   "source": [
    "This contains some changes from train3.ipynb:\n",
    "* an accuracy metric has been implemented\n",
    "* one cycle learning rate schedule is being used\n",
    "* weight tying is disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e84e5-33f4-408f-8a1f-eccb869f813e",
   "metadata": {},
   "source": [
    "This contains some improvements from train4.ipynb: just that the attention module used uses pytorchs implementation for sdpa. This also uses a text generation function to display the capabilities of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5151e-4688-4011-b702-a464ae0a4374",
   "metadata": {},
   "source": [
    "This contains some changes over train5.ipynb. We use data loading hooks for setting up the train/validation data loaders. We also use the setup hook for setting up the gpt model. We also call compile on the gpt model before training. We also have some code for investigating memory leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd2da4",
   "metadata": {},
   "source": [
    "The train6 files were used to determine the cause of the memory leak which seems to have been using multiple workers which causes copy-on-reads to occur. setting num_wokers=0 in the dataloader resolves this issue.\n",
    "See issue: https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662\n",
    "blogpost: https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af82625",
   "metadata": {},
   "source": [
    "The train7 files started using datasets processed by huggingface libraries. This file continues from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba0e74",
   "metadata": {},
   "source": [
    "This file has many improvements over the train8.ipynb files. By default this script uses:\n",
    "* packed sequences\n",
    "* attention masks\n",
    "* a deep model with many layers\n",
    "* mixed precision training\n",
    "* a small vocab of 4096 token ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31007f0",
   "metadata": {},
   "source": [
    "~~This file shows a huge improvement in accuracy and loss. Many possible explanations:~~\n",
    "* ~~Rotational positional encoding was added~~\n",
    "* ~~the dataset used was cleaned of weird symbols (accents, chinese characters, etc.)~~\n",
    "* ~~smaller max_lr~~\n",
    "\n",
    "Huge improvements were false negatives due to bad causal mask construction (data leakage)\n",
    "\n",
    "Further improvements:\n",
    "* apply positions from dataset to RoPE\n",
    "* turn off default positional encodings (or use positions from dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc0125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer_small_cleaned.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ed6a11-1e99-444f-b4b2-7e54873f2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_30M = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": True,\n",
    "    \"no_pos_emb\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c7e0f3-3ba2-4e90-8549-180e8390bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_30M_small_emb = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 256,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 18,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fc4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_60M = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 8,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658334ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_120M_DEEP = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512, # this must be multiple of 64 for the flash attention implementation\n",
    "    \"emb_dim\": 512, # this must be multiple of 16 * n_heads for the flash attention implementation\n",
    "    \"n_heads\": 32,\n",
    "    \"n_layers\": 36,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": True # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a821003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_120M_SHALLOW = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 64,\n",
    "    \"n_layers\": 10,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bea6bd3-62db-4184-83e5-8284297f51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname().lower()\n",
    "if \"laptop\" in hostname:\n",
    "    GPT_CONFIG = GPT_CONFIG_30M\n",
    "else:\n",
    "    GPT_CONFIG = GPT_CONFIG_120M_DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59742e54-fcc7-49a2-b971-66d24cfcd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641a1af-5cd5-4ea0-b10f-6f673dea2840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a88551b-b45a-486d-ad49-c0b8e29f41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"batch_size\": 32 if \"laptop\" in hostname else 32,\n",
    "    \"epochs\": 3,\n",
    "    \"num_workers\": 23,\n",
    "    \"max_lr\": 1e-3 / 2,\n",
    "    \"compile\": \"laptop\" not in hostname\n",
    "}\n",
    "trainer_config[\"grad_batches\"] = 256 // trainer_config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c777bb-c339-47f8-a8d8-2ef7fe3cab6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177bc581-d01c-4691-9e05-2b30b2738808",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gebrial/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from components.gptmodel import GPTModel_RoPE\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import lightning as L\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class LitGPTModel(L.LightningModule):\n",
    "    def __init__(self, trainer_config, gpt_config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.gpt_config = gpt_config\n",
    "        self.trainer_config = trainer_config\n",
    "\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_steps = []\n",
    "        self.learning_rates = []\n",
    "        self.batch_step = 0\n",
    "\n",
    "    def _accuracy(self, output, expected):\n",
    "        total_matching = (torch.argmax(output, dim=-1) == expected).sum().item()\n",
    "        total_numel = expected.numel()\n",
    "        return total_matching / total_numel\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.batch_step += 1\n",
    "\n",
    "        x, y = batch[\"packed_inputs\"][:, :-1], batch[\"packed_inputs\"][:, 1:]\n",
    "        attn_mask = batch[\"attention_mask\"][:, :-1, :-1]\n",
    "        positions = batch[\"padded_positions\"][:, :-1]\n",
    "        logits = self.model([x, attn_mask, positions])\n",
    "\n",
    "        accuracy = self._accuracy(logits, y)\n",
    "        self.log(\"accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.train_accuracy.append(accuracy)\n",
    "\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        current_lr = self.optimizers().param_groups[0][\"lr\"]\n",
    "        self.learning_rates.append(current_lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.val_steps.append(self.batch_step)\n",
    "        x, y = batch[\"packed_inputs\"][:, :-1], batch[\"packed_inputs\"][:, 1:]\n",
    "        attn_mask = batch[\"attention_mask\"][:, :-1, :-1]\n",
    "        positions = batch[\"padded_positions\"][:, :-1]\n",
    "        logits = self.model([x, attn_mask, positions])\n",
    "\n",
    "        accuracy = self._accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.val_accuracy.append(accuracy)\n",
    "\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def loss(self, output, expected):\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            output.flatten(0, 1), expected.flatten()\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.trainer_config[\"max_lr\"], weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.trainer_config[\"max_lr\"],\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"monitor\": \"loss\"\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler_config\n",
    "        }\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.packed_dataset = load_from_disk(\"packed_dataset_with_mask_smallVocab_cleaned\")\n",
    "        self.packed_dataset.set_format('torch')\n",
    "\n",
    "    def configure_model(self):\n",
    "        # check if model is already created\n",
    "        if hasattr(self, \"model\"):\n",
    "            return\n",
    "\n",
    "        self.model = GPTModel_RoPE(self.gpt_config)\n",
    "        if self.trainer_config[\"compile\"]:\n",
    "            self.model = torch.compile(self.model, fullgraph=True)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.packed_dataset[\"train\"],\n",
    "            batch_size=self.trainer_config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=self.trainer_config[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.packed_dataset[\"validation\"],\n",
    "            batch_size=self.trainer_config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=self.trainer_config[\"num_workers\"],\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True,\n",
    "            prefetch_factor=2,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac4e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e1c344a-57df-4d89-9369-b177fb873022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=0-step=3770.ckpt\n",
      "Loaded model from ./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=1-step=7540.ckpt\n",
      "Loaded model from ./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=2-step=11310.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint_paths = [\n",
    "    \"./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=0-step=3770.ckpt\",\n",
    "    \"./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=1-step=7540.ckpt\",\n",
    "    \"./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/epoch=2-step=11310.ckpt\",\n",
    "]\n",
    "models = []\n",
    "for path in checkpoint_paths:\n",
    "    loaded_model = LitGPTModel.load_from_checkpoint(\n",
    "        checkpoint_path=path,\n",
    "    )\n",
    "    models.append(loaded_model)\n",
    "    print(f\"Loaded model from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9f9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_model = LitGPTModel.load_from_checkpoint(\n",
    "  checkpoint_path=checkpoint_paths[0],\n",
    ")\n",
    "avg_state_dict = averaged_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45618866",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in avg_state_dict.keys():\n",
    "    if avg_state_dict[key].dtype.is_floating_point:\n",
    "        avg_state_dict[key].data = torch.mean(\n",
    "            torch.stack([model.state_dict()[key].data for model in models], dim=0),\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "averaged_model.load_state_dict(avg_state_dict)\n",
    "litmodel = averaged_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd0a0a-0864-4340-83e0-906c65ea6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2c30-2a2c-480c-b2c9-568372d957d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a312c6-dfc5-4a71-9599-67384fa7375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import decoders\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer_small_cleaned.json\")\n",
    "tokenizer.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdae804-8fc1-4e23-b02f-ef2afa908114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee44a9-13ed-474b-ae31-363895177b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601fe9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPTModel_RoPE(\n",
       "    (tok_emb): Embedding(4096, 512)\n",
       "    (pos_emb): Embedding(512, 512)\n",
       "    (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "    (trf_blocks): Sequential(\n",
       "      (0): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (12): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (13): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (14): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (15): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (16): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (17): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (18): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (19): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (20): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (21): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (22): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (23): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (24): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (25): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (26): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (27): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (28): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (29): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (30): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (31): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (32): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (33): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (34): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (35): TransformerBlock_RoPE(\n",
       "        (att): MultiHeadAttention_RoPE(\n",
       "          (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (rope): RotaryPositionalEmbeddings()\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layers): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (out_head): Linear(in_features=512, out_features=4096, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "litmodel.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ddc9aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117888000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in litmodel.model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89829428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "daf3b452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  tom and jane are friends. one day, jane goes to tom ' s house. tom has a big pot of soup. he wants to share it with jane. \" jane, do you want some soup?\" tom asks. \" yes, please. it looks yummy,\" jane says. tom p ours some soup into two bowl s. he gives one bowl to jane. jane takes a spoon ful of soup, but then she makes a face. the soup is bitter! jane sp its out it and says : \" this soup is sour!\" tom laughed, and he gave her a hug to make her happy too! jane is so thankful for her friend ' s hug and for giving them both something sweet to eat! the soup is so yummy! they eat the rest together and it was so yummy. they are both so happy!\n"
     ]
    }
   ],
   "source": [
    "from components.generatetext import generate_text_with_attn_positions\n",
    "\n",
    "\n",
    "litmodel.eval()\n",
    "starting_text = \"Tom and Jane are friends. One day, Jane goes to Tom’s house. Tom has a big pot of soup. He wants to share it with Jane. “Jane, do you want some soup?” Tom asks. “Yes, please. It looks yummy,” Jane says. Tom pours some soup into two bowls. He gives one bowl to Jane. Jane takes a spoonful of soup, but then she makes a face. The soup is\"\n",
    "text = generate_text_with_attn_positions(litmodel.model, tokenizer, starting_text, 512, device, topk=3, temperature=1)\n",
    "print(\"text: \", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5760a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  tom and jane are friends. one day, jane goes to tom ' s house. tom has a big pot of soup. he wants to share it with jane. \" jane, do you want some soup?\" tom asks. \" yes, please. it looks yummy,\" jane says. tom p ours some soup into two bowl s. he gives one bowl to jane. jane takes a spoon ful of soup, but then she makes a face. the soup is sour! jane ' ll have to eat the other soup first. tom and jane both drink the bitter water and laugh together at their funny jokes! they both have a great afternoon eating soup and talking about the fun they have.\n"
     ]
    }
   ],
   "source": [
    "from components.generatetext import generate_text_with_attn_positions\n",
    "\n",
    "\n",
    "litmodel.eval()\n",
    "starting_text = \"Tom and Jane are friends. One day, Jane goes to Tom’s house. Tom has a big pot of soup. He wants to share it with Jane. “Jane, do you want some soup?” Tom asks. “Yes, please. It looks yummy,” Jane says. Tom pours some soup into two bowls. He gives one bowl to Jane. Jane takes a spoonful of soup, but then she makes a face. The soup is\"\n",
    "text = generate_text_with_attn_positions(litmodel.model, tokenizer, starting_text, 512, device, topk=3, temperature=1)\n",
    "print(\"text: \", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd42b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1931780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitGPTModel(\n",
       "  (model): OptimizedModule(\n",
       "    (_orig_mod): GPTModel_RoPE(\n",
       "      (tok_emb): Embedding(4096, 512)\n",
       "      (pos_emb): Embedding(512, 512)\n",
       "      (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "      (trf_blocks): Sequential(\n",
       "        (0): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (6): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (7): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (8): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (9): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (10): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (11): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (12): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (13): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (14): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (15): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (16): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (17): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (18): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (19): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (20): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (21): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (22): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (23): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (24): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (25): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (26): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (27): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (28): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (29): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (30): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (31): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (32): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (33): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (34): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (35): TransformerBlock_RoPE(\n",
       "          (att): MultiHeadAttention_RoPE(\n",
       "            (W_query): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_key): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (W_value): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (rope): RotaryPositionalEmbeddings()\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (layers): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (final_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (out_head): Linear(in_features=512, out_features=4096, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35ef4c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 10: One day a girl walked into the living room and noticed something very strange. There was a huge cabinet standing in the corner. It looked very old and heavy. She walked over and tried to open it, when suddenly\n",
      "Row 10: the lid popped shut. she tried to open it again but the cabinet wouldn '. she started to get very scared, so the girl decided to go back to her house to get help to open this cabinet. when the cabinet finally opened, the girl saw lots of interesting things! she was so happy and she started playing in the cabinet all the time. the girl was so happy to be back home with the big, heavy, and exciting things. the cabinet stayed shut and she could explore all of the fun places inside! she had a great adventure in that big and heavy cabinet.\n",
      "Processed 10/300 rows. Estimated time left: 373.67 seconds.\n",
      "Prompt 20: Jack asked his mom if he could ride the bike all the way to his grandmother's house. She agreed, but she said that he shouldn't ride too fast because it was raining and the path was wet.\n",
      "He started riding and realized he was hungry, so he decided that he should get to his grandmother's house as fast as possible. But then he remembered his mum's words \"\n",
      "Row 20: no \", and decided that it was too ris k ful. jack stopped his bike, feeling a bit ashamed, then decided that it was best to wait for her. he asked if he was still hungry so his mom said yes. jack waited patient ately for her to finish cooking, then he finally got to ride to her house and enjoy a snack of his favorite food - cookies and juice. he had learned his lesson and knew he would be more patient next time.\n",
      "Processed 20/300 rows. Estimated time left: 378.43 seconds.\n",
      "Prompt 30: Alice walked up to her friend Ben's house. She was planning to ask him to go to the park with her. When Ben opened the door, she asked him if he had any plans. He said, \"I'm sorry, Alice, but\n",
      "Row 30: we ' ll just stay here and wait.\" alice felt sad and started to walk away. suddenly, she saw something shiny and shiny in a nearby tree! it was a beautiful diamond. she ran up and grabbed the gem. it sparkled like diamond. she smiled with delight. she ran to her mom to show the gem. \" mom,\" alice asked, \" look what we found!\" her mum smiled, \" oh my good, it ' ll be very special to you,\" alice smiled and put her diamond in her bag. \" i hope it will make my dreams even more special \" she whispered, \" i will keep this diamond forever!\" and she did!\n",
      "Processed 30/300 rows. Estimated time left: 382.92 seconds.\n",
      "Prompt 40: Alice walked into the kitchen and saw Ben who was looking for something but looked frustrated. She said, \"Ben, why are you\n",
      "Row 40: frown ing?\" he replied with his frown and alice asked him why he had been frown ing so much and he replied, \" because you were looking for food and you couldn ' t find anything.\" alice felt sad and she said, \" i wanted to help, but there are no food available to eat.\" she then said to herself, \" maybe if you keep looking, you might be happy and i will find some food.\" alice started to search for food but she was still confused and couldn only look at ben ' s sad ness. she kept searching until finally, she spotted some food lying on the table and smiled. she picked up the plate and started eating the food with a smile. alice felt much less confused and she smiled back. from that time on wards, whenever she felt down or sad she remembered ben ' s kindness, and that was the most important lesson of all - that it ' d make you feel better if you just keep searching!\n",
      "Processed 40/300 rows. Estimated time left: 387.12 seconds.\n",
      "Prompt 50: Once upon a time, there was tiger who liked to play the guitar. One day, a bunny heard the guitar from a distance and\n",
      "Row 50: wanted to listen. \" i want a friend to listen to me play the violin,\" said tiger, \" can we be together?\" the rabbit thought for a moment and said,\" yes! we can be together. let me help.\" the two of them went off together to listen. the rabbit was very persistent. she tried to play a few songs and the bunny kept playing the guitar. they sang songs until it was time to go. the rabbit was so happy that he gave tiger a big hug and they said good night.\n",
      "Processed 50/300 rows. Estimated time left: 371.69 seconds.\n",
      "Prompt 60: \"Ben, what do you have in your pocket?\", Alice asked. \"Oh, nothing.\", Ben replied. But Alice saw that there was definitely something in Ben's pocket, and she was very curious what it was, so she\n",
      "Row 60: pulled the hand out and opened her hand. to her delight, there was a beautiful butterfly! she smiled at ben and asked if he could hold the butterfly for a while. \" yes,\" said ben with a twink le in its eyes, as alice gently held the hand out. the two of their fingers were amazed at how beautiful it felt. \" let it go,\" ben exclaimed as the two watched the butterfly fly away, leaving alice smiling with wonder. the two were now best pal m friends. they had seen so many amazing things and shared a special b ou qu e of joy with one another, like the beautiful flowers and trees. they would always look out for each others in need, no longer alone or alone in ben a s pocket.\n",
      "Processed 60/300 rows. Estimated time left: 378.83 seconds.\n",
      "Prompt 70: Jimmy was on his way to school with his father when he noticed a tree with strange looking fruits on it. He asked his father, \"Can I try\n",
      "Row 70: some of these?\" but dad said, he said, a no, those fruit looks bad and it might not taste nice, so we must stay here.\" but jimmy was very persistent and he said he was going. he wanted the fruits so bad that he started running away from the tree, trying not looking for a way. dad was worried, but he knew he could find something else to eat instead, so he said he could come back later when it wasn ' t too dark and he wouldn a t get caught in the bad fruits! jimmy was so happy and he ran home to eat his apples and forget about the bad fruit. he had lots of fun and was very proud to be persistent in his journey home. the moral of his lesson was to always be persistent in life and to never forget what you love and care for.\n",
      "Processed 70/300 rows. Estimated time left: 366.81 seconds.\n",
      "Prompt 80: Jack and Sally were running through the woods together when they stopped suddenly. They heard a strange sound coming from around the corner.\n",
      "“What's that?” asked Jack.\n",
      "\n",
      "Sally put her finger to her lips and said, “Shhhh. Let's go find out.”\n",
      "\n",
      "Jack and Sally followed the\n",
      "Row 80: strange noise. when they arrived they found a big tree that looked like an old tree with leaves all around it, and it looked like a secret. jack was so surprised that sally had heard about it, and they ran to the old tree. they looked at each of the leaves and wondered how the secret had happened. they decided to keep the leaves safe so that they could come back and visit the tree again.\n",
      "Processed 80/300 rows. Estimated time left: 345.03 seconds.\n",
      "Prompt 90: Anna was a popular girl, who walked to school every day carrying a very heavy backpack. It was so heavy that she could hardly walk.\n",
      "One day, Anna's friends asked her why her backpack was so heavy. She said that\n",
      "Row 90: it was full of books. her friends laughed at how much anna had to weigh, and anna felt very embarrassed that they were all so small. but then she remembered that her friends liked her very much, and that she could carry heavy things. so anna put the heavy backpack on her shoulder, took her books and started carrying her backpack around the school, carrying her books everywhere! she was very proud that it felt like a big bag and that she was the best at walking. she felt like a grown - up, and she knew she was the best. she even carried the books to school! anna ' s friends were very proud and she was so proud. from now that her bag was not so heavy and it was much light ier than she expected. she knew she had the strength to carry it and it was the perfect thing to do when it felt too big. anna smiled as the bag fit perfectly.\n",
      "Processed 90/300 rows. Estimated time left: 328.71 seconds.\n",
      "Prompt 100: One day, Grandma was in the kitchen getting ready to prepare. She had the ingredients for a tasty dish.\n",
      "Grandma said to her granddaughter, \"Are you going to help me make this meal?\" Her granddaughter, Jenny, replied, \"Yes, I will help you!\"\n",
      "\n",
      "Grandma and Jenny began to prepare the ingredients. Jenny was clumsy. She dropped\n",
      "Row 100: the flour and the vegetables. she laughed as she mixed them together. grandma said to her, \" you are so clumsy, jenny. but don a t worry, you can do anything you put your hand together. just be gentle!\" jenny nodded, feeling proud that she had helped.\n",
      "Processed 100/300 rows. Estimated time left: 308.78 seconds.\n",
      "Prompt 110: Once upon a time there was a small brown mouse who lived in a big tree. The mouse wanted to be better friends with the birds that flew around the tree, but they were always too afraid to let him get close. \n",
      "One day, the mouse spoke to the birds bravely, \"Please don't be\n",
      "Row 110: afraid of me, i ' m just a small brown bird and i ' d love that!\" the birds were very kind to the mouse, so the bird decided to let him join them and help the mouse learn to be less scared of the big brown bird and his small brown feathers. from then, whenever one was in need of help, they all would come to him for advice, and they were all happy to help each other in times that came their way!\n",
      "Processed 110/300 rows. Estimated time left: 292.32 seconds.\n",
      "Prompt 120: Jack was a chubby little boy. He liked to pick flowers in the garden. One day, he went out to the garden to pick some flowers but his Mom said he had to take his medicine first. He didn't like the medicine's taste.\n",
      "\"Do I have to take it?\" Jack asked.\n",
      "\n",
      "His Mom nodded. \"Yes,\n",
      "Row 120: you can have the pill. just take the pill first and it ' s all gone,\" but jack didn ' ve got ten it. he didn a t want to take a pill because he wanted to keep playing in the garden, not even looking at all the plants around. he started running and jumping and laughing. he didn not see that the medicine had made him sneeze! it hurt his nose and made his nose feel funny and he couldn ' ke his head on the ground! \" oh jack,\" said his mother, looking at the medicine. \" you must be very tired now.\" she took him inside and cleaned his nose, and then gave him the rest he wanted for his medicine - and the rest of his day!\n",
      "Processed 120/300 rows. Estimated time left: 272.86 seconds.\n",
      "Prompt 130: Once there was a mother and daughter walking in the park. They noticed a bird perched on a branch, singing its song. The daughter shouted with delight, “Look Mommy! A bird!”\n",
      "The mother smiled and said, “Yes! See how colorful it is?”\n",
      "The daughter then asked, “Can I share my snack with it?”\n",
      "The mother replied, “No, sweetie. That's a bad idea because\n",
      "Row 130: birds don ate their snacks. you should share them with others.\" the girl was disappointed but agreed with her mommy ' advice. the mother said goodbye to her little one, gave her daughter a hug before they left. as they left the park, the daughter looked up to her mother and smiled. she was so proud of what they did.\n",
      "Processed 130/300 rows. Estimated time left: 254.02 seconds.\n",
      "Prompt 140: Once upon a time there was a little girl called Emma. She was only three years old. One day, her aunt said to her, “Emma, would you like to go out with me to pick up some flowers?”\n",
      "Emma was very excited. She said,\n",
      "Row 140: a yes! i love picking! what kind do you think we can find?\" her aunt said, a le cture s are when something is very difficult and we need our support and care to make the garden better. that is the most difficult task. a emma nodded her head. she was very determined and she said, ' yes, let ' s do it! a they both went outside to look around and emma ' ll have lots to do, while she picked the most beautiful flower and held the garden together with her hands and her feet. after a long day, they were finished. her mum smiled, said to emma that they were done. they both felt so proud and hugged each one. then they walked home together and emma said a thanks for helping me pick flowers, aunt. a\n",
      "Processed 140/300 rows. Estimated time left: 233.92 seconds.\n",
      "Prompt 150: Once upon a time there was a small girl named Lucy. She was only three years old and was very adventurous. She loved to explore the world around her, especially when it was bright and sunny outside. \n",
      "One day, while exploring the nearby park, Lucy came across a ladder leaning on a wall. She was curious to see what's on top, so she climbed the ladder, but when she reached the top, the ladder fell and she was stuck.\n",
      "\n",
      "A nearby park ranger noticed her and shouted out, \"\n",
      "Row 150: lucy! you need to get off the tree right there or i ' d get you stuck.\" lucy knew she had been too curious, and she quickly jumped down from the ladder and ran away. she ran back home, but she was still curious and wanted to explore. that day she decided that she would try to find something else exciting to do and she went out on her way.\n",
      "Processed 150/300 rows. Estimated time left: 218.41 seconds.\n",
      "Prompt 160: Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn't move. Every day, it would say\n",
      "Row 160: \" please, please, please \". but no matter how much someone said, the pumpkin just couldn a pers. one day the pumpkin saw something very interesting in the field. there was an apple tree with many juicy, red apples! it asked the apple tree, but it didn ' t say a word. so the pumpkin decided that it wanted the tree ' apple. the pumpkin started to speak to the tree, saying \" hello tree. i want some apple. can we be friends?\" but then the apple tree spoke up. \" no,\" said it. \" i can only talk to the pumpkin.\" the pumpkin was very sad, so the apple plant said, ' if you don a t be nice, you won ' ll be very lonely '. but then, the apple vine spoke up. he told him that it was not a nice word and that he should always say please. he said that he was lonely, and that it wasn ' t nice to talk to others without being nice and friendly. so from this day forward the pumpkin was kind to the pumpkin plant. he talked to it and said, \" please, can i have a friend?\". and so the pumpkin plant and pumpkin vine were happy ever since, talking, playing, eating apples and having a good time. the end.\n",
      "Processed 160/300 rows. Estimated time left: 206.00 seconds.\n",
      "Prompt 170: Mum was getting ready for bed, so she told her son, \"It's time for a bath\". He nodded and ran off to the bathroom. He watched as Mum turned on the bathtub, the water pouring out of the tap and filling the tub.\n",
      "\n",
      "Mum asked, \"What do you want in your bath?\" He thought for a moment and said\n",
      "Row 170: \" i want to play in it!\". mum smiled. she opened the door to the bathtub. she said, \" go ahead, but don ' t make any mess.\" the son ran to get in. he started to make lots and bubbles with his fingers and feet. the bubbles were so fun and he giggled with delight. he played for a few hours, making big waves and bubbles. when the water was over he said goodbye to mum and ran out of the bathroom.\n",
      "Processed 170/300 rows. Estimated time left: 192.58 seconds.\n",
      "Prompt 180: Once upon a time there was a little boy named Tom. Tom loved playing outside, but he always had to be careful not to make too much noise.\n",
      "One day, Tom went out to explore the garden. Soon he found an unusual fruit. It looked like a prune! Tom picked up the prune and looked at it closely. It smelled sweet.\n",
      "\n",
      "Tom wanted to find out how the prune tasted, so he\n",
      "Row 180: took the fruit home and asked his mom. tom : \" mommy what ' s in my prun es?\" his mom replied. tom : \" they are yummy and sour!\"\n",
      "Processed 180/300 rows. Estimated time left: 175.18 seconds.\n",
      "Prompt 190: One day, Timmy had a goal. He wanted to fit into his favorite toy car. He was very excited, but grumpy too. He tried to fit into the car but he was\n",
      "Row 190: still grumpy. so he decided to take the car outside. he put on a hat to keep him cool. timmy went to his friend ' s garden. he asked her if she wanted to play with his new toy. she was so happy! she said she wanted to fit inside. so, she put in some of the toy cars. they both fit in perfectly. they drove and played until the day was done. timmy felt very proud of what his goal was to fit in his car.\n",
      "Processed 190/300 rows. Estimated time left: 159.96 seconds.\n",
      "Prompt 200: It was a sunny day and Ben wanted to go outside and play. He grabbed his toy car and ran to the garden. Suddenly, something pinched his hand!\n",
      "Ben looked around but there was nothing there. He was scared so he held onto the hoop tightly.\n",
      "\n",
      "Then he heard a voice. \"Don't be scared, little one. It's just me!\"\n",
      "\n",
      "Ben looked up and saw a big, scary spider. He was scared!\n",
      "\n",
      "The spider smiled. \"Don't be scared, I just wanted to\n",
      "Row 200: play with you.\" he then said, the two of them started playing tag together, running around the hoop and laughing. ben had never had such a fun time before. he forgot about the scary spid er and enjoyed the sunshine with his toy car in the garden all day long!\n",
      "Processed 200/300 rows. Estimated time left: 144.93 seconds.\n",
      "Prompt 210: Once there was a boy named Sam. He was three years old and very naughty. One day, his mom told him not to touch her phone but Sam did not listen. He picked up the phone and started playing with it.\n",
      "Mom saw him and said, \"be careful not to\n",
      "Row 210: break it. that is not a good idea!\" sam felt guilty and said he would never do this. but he was still curious and kept on playing, even though his mom told him not. mom said, ' if i say no to the rules, you must listen to me and not do it again. that ' s why you must be pun ished.\" but it didn ' s best. sam kept on playing and ignored her warn ings. he kept playing with her phone, and eventually his mom got angry and told sam off to go to the hospital. the moral was : it is always important for you to listen to your mom, and to not touch things that are dangerous or wrong without asking first.\" sam was sad but knew he should listen and o we mom his warn ings to do as he asked. he promised he would never do it without asking his mother first. from that day on ward he always listened and was very obedient, but also a lot more respectful.\n",
      "Processed 210/300 rows. Estimated time left: 130.36 seconds.\n",
      "Prompt 220: Once upon a time, there was a little boy who was always naughty. His mom was always telling him to be good, but he kept disobeying her rules and ignoring her warnings. \n",
      "One day, he was so naughty that his mom decided to punish him. She told him that he had to\n",
      "Row 220: go to bed without any dinner or else his parents would pun ish him. the little naughty boy was very sad, so the next day he decided not to do what he was asked to do. the moral was that if the little one did not behave well he would be very sad and he was pun ish ment.\n",
      "Processed 220/300 rows. Estimated time left: 114.68 seconds.\n",
      "Prompt 230: Tim and Sam were brothers who liked to play pretend. They had a toy gun that made a loud noise when they pulled the trigger. They liked to pretend they were strong heroes who fought bad guys.\n",
      "One day, they were playing in the backyard when they heard a knock on the door. It was their neighbor, Mrs. Lee, who was very old and kind. She brought them some cookies and asked\n",
      "Row 230: , a what do you think you are up to, boys? you look very tired. a tim said, we were playing knight and i was a hero who saved a prince from the bad drag ons. a sam said he was not tired and wanted a snack. but he did na t want a cookie or an ambulance either, so mrs and mr and mrs.\" tim thought for a moment and then said he wanted some snacks too and he did na v ive. sam thought that would make them feel good. they agreed and went back inside. mrs. lee was happy and said she was very kind. but when mrs. lee came out with the snacks she said they were all gone now. she had a surprise for them : they could have a cookie each. she said she had a real gun that was not real. tim was curious about the gun and asked mrs. rose how it was for them. she told him it had some buttons and a wire that she used for shoot ing. she told him it could make things fly and make noise, or turn things on and on, but it could not be dangerous. tim was amazed and curious about what the pistol could be for and why mrs. rose had a real pistol that made loud noises. sam thought that maybe he should not play with the pistol, but he was also hungry and wanted a real gun too! he said to mr. lee, a maybe we should ask mr. rose for a snack. she is very wise, but also a good person. we should be careful with our gun s, they are not real gun s. we should only play pretend, but not pretend.\" mrs. jen agreed to the plan. she gave them a real pistol and a candy each and told them to play pretend with them. they thanked mrs and mrs and took the candy to mr, who was watching tv. they made loud noises and waved their pistol s around the room. mr. lee laughed at their silly noises and joined in. he pretended to shoot at them, pretending they could fly and shoot bad gu es ses\n",
      "Processed 230/300 rows. Estimated time left: 104.22 seconds.\n",
      "Prompt 240: Anna and Ben are playing in the park. They like to pretend they are pirates and look for treasure in the sand. They have a big bucket and a small shovel. They dig and dig and find many things: a shiny stone, a red feather, a broken toy car.\n",
      "\"Look, Anna, I found a gold coin!\" Ben says, holding up the stone.\n",
      "\"Wow, Ben, you are so lucky!\" Anna says. \"Maybe we can buy\n",
      "Row 240: something with the coin!\" anna and they run back and look at the coin and the stone again, hoping for something good. but they cannot see what it is or how much money it has in the car. they see some kids throwing rocks at a trash bag and throwing rocks at each other. anna and anna are angry. the coin is dirty and old. anna has an op in ion. ben is right, they are not rich, she thinks the rock bag was trash! they think it is a good thing. \" hey ben, why did the kids throw stones in the trash can?\" anna asks, frown ning. \" they don ' t care about the rock. we only think they are treasure!\" \" ye ah, we are not treasure!\" anna shouts back, frown ning at him with her face.\" \" they have a lot of value. the coin is not worth lo ses.\" ben looks at anna. he sees that anna is right and sad. they don a t matter if the treasure is not worth anything. \" i am sorry, too!\" anna and the kids apolog ize. \" you are right. we were wrong to keep a treasure. but we are still lucky to have each real gold coin.\" they hug anna ' s leg and each other. they decide not to be rich and mean to each others, but they also think about the coin. they are still happy, but not more gold and rocks.\n",
      "Processed 240/300 rows. Estimated time left: 94.22 seconds.\n",
      "Prompt 250: Tom and Mia are friends. They like to play with their toys. Today, they have many vehicles to play with. They have cars, trucks, buses, and trains.\n",
      "\"Look, Mia, I have a red car. It is very fast. Vroom, vroom!\" Tom says.\n",
      "\"I have a yellow bus. It is very big. Beep, beep!\" Mia says.\n",
      "They make noises and move their\n",
      "Row 250: toys on their cars and trains. \" vroom, vroom, vroom, beep!\" they laugh and have more fun. they play until their mom comes to pick her. they are tired and hungry, so they put their toys in the car. mom comes to their house and smiles when they see them playing. she sees their vehic le and smiles. \" what is that, tom? is that my car?\" mom asked, pointing to a red light on a tray on their door. they run outside and look. they do see their mom. \" wow, that car is amazing. where is it?\" tom asks. mom points to the window where the sun is shining on it. \" it ' s noon. the weather is good for us and for the sun.\" mia says and points at the sun too. they hug mom again. \" can i drive your car, mom?\" tom says and gets into the driver ' s seat. they push their cars and trucks on a seat and make noises with the horn. mom dri f ts her car and tom and lily zoom around their yard. \" beep beep, beep hon!\" they say together, making their cars go faster and faster on the road.\n",
      "Processed 250/300 rows. Estimated time left: 81.67 seconds.\n",
      "Prompt 260: Lily and Tom are friends. They like to play with toys. Lily has a big tank. It is green and has wheels. Tom has a small car. It is red and has doors.\n",
      "One day, they go to the park. They bring their toys. They find a hill. They want to race their toys down the hill. Lily says, \"My tank is faster than your car. It can go over rocks and grass.\" Tom says, \"No, my car is faster than your tank. It can go \n",
      "Row 260: faster and far ther. it will win!\" they argue. they push each other and shout. they make loud noises. then, lily ' s tank h its tom on the arm. tom cries out loud too and falls down the hill too fast. his car lands near a rock and h its his tank on a rock. his car stops and stops moving. tom is very sad and angry. lily ' sses tom ' s horn to say hello. but tom is still angry and pushes her. lily falls down and hurts tom ' s head. she cries and cries louder and harder too much.\" lily ' sses tom ' arm too. she wants to say sorry and be friends. tom stops and turns away too fast too. lily runs away and leaves the hill with tom. tom feels very lonely in lily ' s car. they hug their cars. they wish they could be friends.\n",
      "Processed 260/300 rows. Estimated time left: 67.26 seconds.\n",
      "Prompt 270: Once upon a time, there was a happy little hedgehog named Spike. Spike loved to play with his friends in the forest. One day, Spike and his friends put on a show for all the animals in the forest. They sang and danced and did tricks. When they were done, all the animals applauded and cheered.\n",
      "But then, a mean old fox came along and said, \"That was a terrible show! You're all terrible performers!\" Spike and his friends\n",
      "Row 270: didn a king at him, but they knew he wasn ' t nice at the party either. so spike said sorry to all the other animals and asked them for a new trick on them. the fox thought for a minute, then smiled. \" i have another trick for the other creatures!\" he whispered back, \" i have a special trick that will make everyone happy again.\" spike and his new friends clapped and cheered, happy that their friend liked his trick. they continued to play in peace, but spike knew it wasn '.\n",
      "Processed 270/300 rows. Estimated time left: 49.69 seconds.\n",
      "Prompt 280: Once upon a time there was a little girl named Sophie. Sophie was playing with her best friend in the park and they were very happy. Suddenly, it started raining. Sophie was worried because she was getting wet. She looked up at the sky and said \"Oh no!\"\n",
      "Sophie's dad came to pick her up. He opened an umbrella and put it over Sophie and her friend. They ran through the rain, back to the car.\n",
      "Once they were in the car and safe from the rain, Sophie's dad said,\n",
      "Row 280: \" don ' t worry sophie, we ' re safe now.\" he gave her an ice cream to make her happy again and she ate the yummy ice - cold treat while her parents drove away from their wet rain coats.\n",
      "Processed 280/300 rows. Estimated time left: 32.49 seconds.\n",
      "Prompt 290: Once upon a time there was a little girl named Suzie. She was walking through the woods, looking for something cool and fun. As she was walking, she stumbled over a twig. She was so surprised, she let out a little giggle. Suzie picked up the twig and started looking around for something else cool.\n",
      "She found a little river with a bunch of pretty stones all around it. She decided to go pick them up. She had so much fun looking for the coolest stones. Suddenly, she stumbled over a big rock. She\n",
      "Row 290: was scared but she decided it was okay to pick up any stones. she put them back into the cool river. su zy smiled and said goodbye. then she continued on her journey, looking around for more stones.\n",
      "Processed 290/300 rows. Estimated time left: 16.03 seconds.\n",
      "Prompt 300: Once upon a time, in a small village, there lived a group of animals. There was a big cat, a little dog, and a tiny bird. They all liked to play together, but one day, they had a big fight. The cat wanted to eat a wet mint leaf, the dog wanted to play with it, and the bird wanted to build a nest with it.\n",
      "The three friends didn't know what to do. They all wanted the wet mint leaf for themselves. Then, a wise old turtle came\n",
      "Row 300: by with some yummy bugs. \" why don ' ts don a ke?\" the owl asked. all three animals thought hard and decided they should all work hard together. the wise owl told the animals to un ited. they all worked hard and built a small house for the wet min ct leaves. the cats, the bird, and the mouse worked very hard to build the small houses, and soon, they all had enough food. they learned that it ' s better to share than be angry. and they all became the wise old owl who loved to play together in their little village every day. the end!\n",
      "Processed 300/300 rows. Estimated time left: 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "with open('evaluation_prompts.csv', mode='r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    data = [row for row in csv_reader]  # Each row is a dictionary\n",
    "\n",
    "data_len = len(data)\n",
    "current_index = 0\n",
    "# measure time\n",
    "start_time = time.time()\n",
    "# Modify data (e.g., change 'age' column to integers)\n",
    "for row in data:\n",
    "    current_index += 1\n",
    "    starting_text = row['prompt']\n",
    "    output_text = generate_text_with_attn_positions(litmodel.model, tokenizer, starting_text, 512, device, topk=3, temperature=1, output_only=True)\n",
    "    row['completion'] = output_text\n",
    "    if current_index % 10 == 0:\n",
    "        # print prompt\n",
    "        print(f\"Prompt {current_index}: {starting_text}\")\n",
    "        # print output\n",
    "        print(f\"Row {current_index}: {output_text}\")\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        time_left = (data_len - current_index) * (elapsed_time / current_index)\n",
    "        print(f\"Processed {current_index}/{data_len} rows. Estimated time left: {time_left:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8070d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./evaluation_outputs_smallVocabCleaned_RoPE_combined.csv', mode='w', newline='') as file:\n",
    "    fieldnames = data[0].keys()  # Get column names\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8f99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "2025-04-10 00:09:51.990430: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-10 00:09:52.116343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744268992.161147  199424 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744268992.174880  199424 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744268992.279719  199424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744268992.279735  199424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744268992.279737  199424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744268992.279738  199424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-10 00:09:52.293463: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 304/304 [03:35<00:00,  1.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_accuracy_epoch     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6777998805046082     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_loss_epoch       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.182990550994873     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_accuracy_epoch    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6777998805046082    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_loss_epoch      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.182990550994873    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = L.Trainer(inference_mode=False)\n",
    "trainer.validate(\n",
    "  model=litmodel\n",
    "\n",
    ")\n",
    "trainer.save_checkpoint(\n",
    "    \"./checkpoints/120M_DEEP_smallVocabCleaned_RoPE2/combined.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11f7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
