{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c22a32f-cb2c-47cd-96fc-fea8953ac491",
   "metadata": {},
   "source": [
    "This script contains some improvement from train.ipynb by default:\n",
    "* much few layers, heads, and embedding dimension to reduce the model size\n",
    "* dataloader v2 which uses a custom tokenizer (again to reduce model size)\n",
    "* no positional embeddings (to reduce model complexity)\n",
    "* weight tying (to reduce model size)\n",
    "\n",
    "\n",
    "We implemented a few things here first and not before:\n",
    "* validation losses\n",
    "* increased the model size to be just below 30M parameters\n",
    "* reduced the amount of data trained with to keep the training (wall) time consistent\n",
    "* made graph more informative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce834d77-2417-42fc-b17f-d30e90267ea7",
   "metadata": {},
   "source": [
    "This script contains a couple improvements from train2.ipynb:\n",
    "* gradient accumulation is enabled\n",
    "* the dataloader chunks from the start of an example up to the max_length or the endoftext token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cab6bf-b4da-4f70-8c24-b249c7645936",
   "metadata": {},
   "source": [
    "This contains some changes from train3.ipynb:\n",
    "* an accuracy metric has been implemented\n",
    "* one cycle learning rate schedule is being used\n",
    "* weight tying is disabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e84e5-33f4-408f-8a1f-eccb869f813e",
   "metadata": {},
   "source": [
    "This contains some improvements from train4.ipynb: just that the attention module used uses pytorchs implementation for sdpa. This also uses a text generation function to display the capabilities of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5151e-4688-4011-b702-a464ae0a4374",
   "metadata": {},
   "source": [
    "This contains some changes over train5.ipynb. We use data loading hooks for setting up the train/validation data loaders. We also use the setup hook for setting up the gpt model. We also call compile on the gpt model before training. We also have some code for investigating memory leaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd2da4",
   "metadata": {},
   "source": [
    "The train6 files were used to determine the cause of the memory leak which seems to have been using multiple workers which causes copy-on-reads to occur. setting num_wokers=0 in the dataloader resolves this issue.\n",
    "See issue: https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662\n",
    "blogpost: https://ppwwyyxx.com/blog/2022/Demystify-RAM-Usage-in-Multiprocess-DataLoader/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af82625",
   "metadata": {},
   "source": [
    "The train7 files started using datasets processed by huggingface libraries. This file continues from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")\n",
    "vocab_size = tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed6a11-1e99-444f-b4b2-7e54873f2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_30M = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": True,\n",
    "    \"no_pos_emb\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7e0f3-3ba2-4e90-8549-180e8390bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_30M_small_emb = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 256,\n",
    "    \"n_heads\": 16,\n",
    "    \"n_layers\": 18,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_60M = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 8,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658334ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_120M_DEEP = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512, # this must be multiple of 64 for the flash attention implementation\n",
    "    \"emb_dim\": 512, # this must be multiple of 16 * n_heads for the flash attention implementation\n",
    "    \"n_heads\": 32,\n",
    "    \"n_layers\": 28,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_120M_SHALLOW = {\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"context_length\": 512,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 64,\n",
    "    \"n_layers\": 10,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"weight_tying\": False,\n",
    "    \"no_pos_emb\": False # conflicts with sequence packing\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea6bd3-62db-4184-83e5-8284297f51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "hostname = socket.gethostname().lower()\n",
    "if \"laptop\" in hostname:\n",
    "    GPT_CONFIG = GPT_CONFIG_30M\n",
    "else:\n",
    "    GPT_CONFIG = GPT_CONFIG_120M_DEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59742e54-fcc7-49a2-b971-66d24cfcd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9641a1af-5cd5-4ea0-b10f-6f673dea2840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88551b-b45a-486d-ad49-c0b8e29f41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = {\n",
    "    \"dataset_scale\": 1,\n",
    "    \"batch_size\": 32 if \"laptop\" in hostname else 16,\n",
    "    \"epochs\": 1,\n",
    "    \"train_file_loc\": \"../data/TinyStories/TinyStoriesV2-GPT4-train.txt\",\n",
    "    \"valid_file_loc\": \"../data/TinyStories/TinyStoriesV2-GPT4-valid.txt\",\n",
    "    \"num_workers\": 0,\n",
    "    \"max_lr\": 1e-2,\n",
    "    \"compile\": False # \"laptop\" not in hostname\n",
    "}\n",
    "trainer_config[\"grad_batches\"] = 256 // trainer_config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c777bb-c339-47f8-a8d8-2ef7fe3cab6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177bc581-d01c-4691-9e05-2b30b2738808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.gptmodel import GPTModel_GeGLU\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import lightning as L\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class LitGPTModel(L.LightningModule):\n",
    "    def __init__(self, trainer_config, gpt_config):\n",
    "        super().__init__()\n",
    "        self.gpt_config = gpt_config\n",
    "        self.trainer_config = trainer_config\n",
    "\n",
    "        self.train_accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_steps = []\n",
    "        self.learning_rates = []\n",
    "        self.batch_step = 0\n",
    "\n",
    "    def _accuracy(self, output, expected):\n",
    "        total_matching = (torch.argmax(output, dim=-1) == expected).sum().item()\n",
    "        total_numel = expected.numel()\n",
    "        return total_matching / total_numel\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.batch_step += 1\n",
    "\n",
    "        x, y = batch[\"packed_inputs\"][:, :-1], batch[\"packed_inputs\"][:, 1:]\n",
    "        attn_mask = batch[\"attention_mask\"][:, :-1, :-1]\n",
    "        logits = self.model([x, attn_mask])\n",
    "\n",
    "        accuracy = self._accuracy(logits, y)\n",
    "        self.log(\"accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.train_accuracy.append(accuracy)\n",
    "\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        current_lr = self.optimizers().param_groups[0][\"lr\"]\n",
    "        self.learning_rates.append(current_lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.val_steps.append(self.batch_step)\n",
    "        x, y = batch[\"packed_inputs\"][:, :-1], batch[\"packed_inputs\"][:, 1:]\n",
    "        attn_mask = batch[\"attention_mask\"][:, :-1, :-1]\n",
    "\n",
    "        logits = self.model([x, attn_mask])\n",
    "\n",
    "        accuracy = self._accuracy(logits, y)\n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.val_accuracy.append(accuracy)\n",
    "\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        self.val_losses.append(loss.item())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def loss(self, output, expected):\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            output.flatten(0, 1), expected.flatten()\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), lr=self.trainer_config[\"max_lr\"], weight_decay=0.1\n",
    "        )\n",
    "\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.trainer_config[\"max_lr\"],\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"monitor\": \"loss\"\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": lr_scheduler_config\n",
    "        }\n",
    "\n",
    "    def setup(self, stage):\n",
    "        self.packed_dataset = load_from_disk(\"packed_dataset_with_mask\")\n",
    "        self.packed_dataset.set_format('torch')\n",
    "\n",
    "        self.model = GPTModel_GeGLU(self.gpt_config)\n",
    "        if self.trainer_config[\"compile\"]:\n",
    "            self.model = torch.compile(self.model, fullgraph=True)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.packed_dataset[\"train\"], batch_size=self.trainer_config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.packed_dataset[\"validation\"], batch_size=self.trainer_config[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af01d54-1a9d-484d-bd23-573bb249cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "litmodel = LitGPTModel(\n",
    "    trainer_config,\n",
    "    GPT_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8b447-2a4d-4eaf-aeef-fc98a417e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=trainer_config[\"epochs\"],\n",
    "    logger=False,\n",
    "    enable_progress_bar=True,\n",
    "    accumulate_grad_batches=trainer_config[\"grad_batches\"],\n",
    "    gradient_clip_val=1.0,\n",
    "    enable_checkpointing=True,\n",
    "    # callbacks=[\n",
    "    #     L.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "    #     L.callbacks.ModelCheckpoint(\n",
    "    #         monitor=\"val_loss\",\n",
    "    #         mode=\"min\",\n",
    "    #         save_top_k=1,\n",
    "    #         filename=\"{epoch:02d}-{val_loss:.2f}\",\n",
    "    #         dirpath=\"./checkpoints/120M_DEEP/\"\n",
    "    #     )\n",
    "    # ],\n",
    "    precision=\"bf16-mixed\" #\"bf16-mixed\", # or \"transformer-engine\" for 8-bit mixed\n",
    ")\n",
    "trainer.fit(model=litmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_checkpoint(\n",
    "#     \"./checkpoints/120M_DEEP_one_epoch.ckpt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac4e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a5eaef-72ed-4086-9f39-52de8fecda65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.plot(litmodel.train_losses, label=\"Training Loss\", color=\"blue\")\n",
    "ax1.scatter(litmodel.val_steps, litmodel.val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "ax1.set_xlabel(\"Training Step\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(litmodel.learning_rates, label=\"Learning Rate\", color=\"green\")\n",
    "ax2.set_ylabel(\"Learning Rate\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Training/Validation Loss and Learning Rate\")\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12848dbc-0678-43d4-a13b-12ce7be0a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(litmodel.train_accuracy, color=\"blue\")\n",
    "plt.scatter(litmodel.val_steps, litmodel.val_accuracy, color=\"red\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c344a-57df-4d89-9369-b177fb873022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dd0a0a-0864-4340-83e0-906c65ea6a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2c30-2a2c-480c-b2c9-568372d957d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a312c6-dfc5-4a71-9599-67384fa7375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdae804-8fc1-4e23-b02f-ef2afa908114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders\n",
    "tokenizer.decoder = decoders.WordPiece()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ee44a9-13ed-474b-ae31-363895177b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e03bec4-4b3d-4c44-96fa-79ce95b73979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "litmodel.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93598d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([p.numel() for p in litmodel.model.parameters() if p.requires_grad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42120a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from components.generatetext import generate_text_with_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458f73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7083a-63f2-4da5-abad-a9acd3f94414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "litmodel.eval()\n",
    "starting_text = \"Tom and Jane are friends. One day, Jane goes to Tom’s house. Tom has a big pot of soup. He wants to share it with Jane. “Jane, do you want some soup?” Tom asks. “Yes, please. It looks yummy,” Jane says. Tom pours some soup into two bowls. He gives one bowl to Jane. Jane takes a spoonful of soup, but then she makes a face. The soup is\"\n",
    "text = generate_text_with_attn(litmodel.model, tokenizer, starting_text, 512, device, topk=3, temperature=1)\n",
    "print(\"text: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd42b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1931780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "with open('evaluation_prompts.csv', mode='r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    data = [row for row in csv_reader]  # Each row is a dictionary\n",
    "\n",
    "data_len = len(data)\n",
    "current_index = 0\n",
    "# measure time\n",
    "start_time = time.time()\n",
    "# Modify data (e.g., change 'age' column to integers)\n",
    "for row in data:\n",
    "    current_index += 1\n",
    "    starting_text = row['prompt']\n",
    "    output_text = generate_text_with_attn(litmodel.model, tokenizer, starting_text, 512, device, topk=3, temperature=1, output_only=True)\n",
    "    row['completion'] = output_text\n",
    "    if current_index % 10 == 0:\n",
    "        # print prompt\n",
    "        print(f\"Prompt {current_index}: {starting_text}\")\n",
    "        # print output\n",
    "        print(f\"Row {current_index}: {output_text}\")\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        time_left = (data_len - current_index) * (elapsed_time / current_index)\n",
    "        print(f\"Processed {current_index}/{data_len} rows. Estimated time left: {time_left:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('evaluation_outputs.csv', mode='w', newline='') as file:\n",
    "    fieldnames = data[0].keys()  # Get column names\n",
    "    csv_writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f99db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
