{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2581d-1eab-4f7b-b647-96f4dd50e7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e5a12-136c-47ec-bf76-d005a2857105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_next_token(tokens, model, device, temperature=1, topk=1):\n",
    "    if temperature == 0:\n",
    "        temperature = 1\n",
    "        topk = 1\n",
    "\n",
    "    if topk < 1:\n",
    "        raise ValueError(\"topk must be >= 1\")\n",
    "\n",
    "    logits = model([tokens, None])[0][-1] / temperature\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    next_token_pos = torch.multinomial(top_logits, num_samples=1)\n",
    "    return top_pos[next_token_pos]\n",
    "\n",
    "def generate_tokens(model, start_tokens, max_length, device, eot_token, temperature=1, topk=1):\n",
    "    final_tokens = torch.full(size=(1, max_length), fill_value=eot_token, device=device)\n",
    "\n",
    "    idx = len(start_tokens)\n",
    "    final_tokens[0][:idx] = start_tokens\n",
    "    while idx < max_length:\n",
    "        tokens = final_tokens[:, :idx]\n",
    "        next_token = generate_next_token(tokens, model, device, temperature=temperature, topk=topk)\n",
    "        final_tokens[0][idx] = next_token\n",
    "        idx += 1\n",
    "\n",
    "        if next_token == eot_token:\n",
    "            break\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, start_text, max_length, device, eot_string=\"<|endoftext|>\", temperature=1, topk=1):\n",
    "    \"\"\"\n",
    "    Ensure tokenizer decode is set to tokenizers.decoders.WordPiece() for best results\n",
    "    max_length refers to number of tokens\n",
    "    device can be either \"cpu\" or \"cuda\"\n",
    "    topk must be >= 1\n",
    "    \"\"\"\n",
    "    eot_token = tokenizer.encode(eot_string).ids[0]\n",
    "    start_tokens = tokenizer.encode(start_text).ids\n",
    "    start_tokens = torch.tensor(start_tokens).to(device)\n",
    "    tokens = generate_tokens(model, start_tokens, max_length, device, eot_token, temperature=temperature, topk=topk)\n",
    "    return tokenizer.decode(tokens[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4191d-9ec9-4c86-9e51-b35f929267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_next_token_with_attn(tokens, model, device, temperature=1, topk=1):\n",
    "    if temperature == 0:\n",
    "        temperature = 1\n",
    "        topk = 1\n",
    "\n",
    "    if topk < 1:\n",
    "        raise ValueError(\"topk must be >= 1\")\n",
    "\n",
    "    full_attn_mask = torch.ones((1, len(tokens[0]), len(tokens[0])), device=device)\n",
    "    logits = model([tokens, full_attn_mask])\n",
    "    logits = logits[0][-1] / temperature\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    next_token_pos = torch.multinomial(top_logits, num_samples=1)\n",
    "    return top_pos[next_token_pos]\n",
    "\n",
    "def generate_tokens_with_attn(model, start_tokens, max_length, device, eot_token, temperature=1, topk=1):\n",
    "    final_tokens = torch.full(size=(1, max_length), fill_value=eot_token, device=device)\n",
    "\n",
    "    idx = len(start_tokens)\n",
    "    final_tokens[0][:idx] = start_tokens\n",
    "    while idx < max_length:\n",
    "        tokens = final_tokens[:, :idx]\n",
    "        next_token = generate_next_token_with_attn(tokens, model, device, temperature=temperature, topk=topk)\n",
    "        final_tokens[0][idx] = next_token\n",
    "        idx += 1\n",
    "\n",
    "        if next_token == eot_token:\n",
    "            break\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def generate_text_with_attn(model, tokenizer, start_text, max_length, device, eot_string=\"<|endoftext|>\", temperature=1, topk=1, output_only=False):\n",
    "    \"\"\"\n",
    "    Ensure tokenizer decode is set to tokenizers.decoders.WordPiece() for best results\n",
    "    max_length refers to number of tokens\n",
    "    device can be either \"cpu\" or \"cuda\"\n",
    "    topk must be >= 1\n",
    "    \"\"\"\n",
    "    eot_token = tokenizer.encode(eot_string).ids[0]\n",
    "    start_tokens = tokenizer.encode(start_text).ids\n",
    "    start_tokens_len = len(start_tokens)\n",
    "    start_tokens = torch.tensor(start_tokens).to(device)\n",
    "    tokens = generate_tokens_with_attn(model, start_tokens, max_length, device, eot_token, temperature=temperature, topk=topk)\n",
    "    if output_only:\n",
    "        return tokenizer.decode(tokens[0][start_tokens_len:].tolist())\n",
    "    return tokenizer.decode(tokens[0].tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
