{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb2581d-1eab-4f7b-b647-96f4dd50e7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e5a12-136c-47ec-bf76-d005a2857105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_next_token(tokens, model, device, temperature=1, topk=1):\n",
    "    if temperature == 0:\n",
    "        temperature = 1\n",
    "        topk = 1\n",
    "\n",
    "    if topk < 1:\n",
    "        raise ValueError(\"topk must be >= 1\")\n",
    "\n",
    "    logits = model([tokens, None])[0][-1] / temperature\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    next_token_pos = torch.multinomial(top_logits, num_samples=1)\n",
    "    return top_pos[next_token_pos]\n",
    "\n",
    "def generate_tokens(model, start_tokens, max_length, device, eot_token, temperature=1, topk=1):\n",
    "    final_tokens = torch.full(size=(1, max_length), fill_value=eot_token, device=device)\n",
    "\n",
    "    idx = len(start_tokens)\n",
    "    final_tokens[0][:idx] = start_tokens\n",
    "    while idx < max_length:\n",
    "        tokens = final_tokens[:, :idx]\n",
    "        next_token = generate_next_token(tokens, model, device, temperature=temperature, topk=topk)\n",
    "        final_tokens[0][idx] = next_token\n",
    "        idx += 1\n",
    "\n",
    "        if next_token == eot_token:\n",
    "            break\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, start_text, max_length, device, eot_string=\"<|endoftext|>\", temperature=1, topk=1):\n",
    "    \"\"\"\n",
    "    Ensure tokenizer decode is set to tokenizers.decoders.WordPiece() for best results\n",
    "    max_length refers to number of tokens\n",
    "    device can be either \"cpu\" or \"cuda\"\n",
    "    topk must be >= 1\n",
    "    \"\"\"\n",
    "    eot_token = tokenizer.encode(eot_string).ids[0]\n",
    "    start_tokens = tokenizer.encode(start_text).ids\n",
    "    start_tokens = torch.tensor(start_tokens).to(device)\n",
    "    tokens = generate_tokens(model, start_tokens, max_length, device, eot_token, temperature=temperature, topk=topk)\n",
    "    return tokenizer.decode(tokens[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4191d-9ec9-4c86-9e51-b35f929267e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_next_token_with_attn(tokens, model, device, temperature=1, topk=1):\n",
    "    if temperature == 0:\n",
    "        temperature = 1\n",
    "        topk = 1\n",
    "\n",
    "    if topk < 1:\n",
    "        raise ValueError(\"topk must be >= 1\")\n",
    "\n",
    "    full_attn_mask = torch.ones((1, len(tokens[0]), len(tokens[0])), device=device)\n",
    "    logits = model([tokens, full_attn_mask])\n",
    "    logits = logits[0][-1] / temperature\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    next_token_pos = torch.multinomial(top_logits, num_samples=1)\n",
    "    return top_pos[next_token_pos]\n",
    "\n",
    "def generate_tokens_with_attn(model, start_tokens, max_length, device, eot_token, temperature=1, topk=1):\n",
    "    final_tokens = torch.full(size=(1, max_length), fill_value=eot_token, device=device)\n",
    "\n",
    "    idx = len(start_tokens)\n",
    "    final_tokens[0][:idx] = start_tokens\n",
    "    while idx < max_length:\n",
    "        tokens = final_tokens[:, :idx]\n",
    "        next_token = generate_next_token_with_attn(tokens, model, device, temperature=temperature, topk=topk)\n",
    "        final_tokens[0][idx] = next_token\n",
    "        idx += 1\n",
    "\n",
    "        if next_token == eot_token:\n",
    "            break\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def generate_text_with_attn(model, tokenizer, start_text, max_length, device, eot_string=\"<|endoftext|>\", temperature=1, topk=1, output_only=False):\n",
    "    \"\"\"\n",
    "    Ensure tokenizer decode is set to tokenizers.decoders.WordPiece() for best results\n",
    "    max_length refers to number of tokens\n",
    "    device can be either \"cpu\" or \"cuda\"\n",
    "    topk must be >= 1\n",
    "    \"\"\"\n",
    "    eot_token = tokenizer.encode(eot_string).ids[0]\n",
    "    start_tokens = tokenizer.encode(start_text).ids\n",
    "    start_tokens_len = len(start_tokens)\n",
    "    start_tokens = torch.tensor(start_tokens).to(device)\n",
    "    tokens = generate_tokens_with_attn(model, start_tokens, max_length, device, eot_token, temperature=temperature, topk=topk)\n",
    "    if output_only:\n",
    "        return tokenizer.decode(tokens[0][start_tokens_len:].tolist())\n",
    "    return tokenizer.decode(tokens[0].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e606e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_next_token_with_attn_positions(tokens, model, device, temperature=1, topk=1):\n",
    "    if temperature == 0:\n",
    "        temperature = 1\n",
    "        topk = 1\n",
    "\n",
    "    if topk < 1:\n",
    "        raise ValueError(\"topk must be >= 1\")\n",
    "\n",
    "    full_attn_mask = torch.ones((1, len(tokens[0]), len(tokens[0])), device=device)\n",
    "    positions = torch.arange(len(tokens[0]), device=device).unsqueeze(0).expand_as(tokens)\n",
    "\n",
    "    logits = model([tokens, full_attn_mask, positions])\n",
    "    logits = logits[0][-1] / temperature\n",
    "    top_logits, top_pos = torch.topk(logits, topk)\n",
    "    top_softmax = torch.softmax(top_logits, dim=-1)\n",
    "    next_token_pos = torch.multinomial(top_softmax, num_samples=1)\n",
    "    return top_pos[next_token_pos]\n",
    "\n",
    "def generate_tokens_with_attn_positions(model, start_tokens, max_length, device, eot_token, temperature=1, topk=1):\n",
    "    final_tokens = torch.full(size=(1, max_length), fill_value=eot_token, device=device)\n",
    "\n",
    "    idx = len(start_tokens)\n",
    "    final_tokens[0][:idx] = start_tokens\n",
    "    while idx < max_length:\n",
    "        tokens = final_tokens[:, :idx]\n",
    "        next_token = generate_next_token_with_attn_positions(tokens, model, device, temperature=temperature, topk=topk)\n",
    "        final_tokens[0][idx] = next_token\n",
    "        idx += 1\n",
    "\n",
    "        if next_token == eot_token:\n",
    "            break\n",
    "\n",
    "    return [final_tokens[0][:idx - 1]]\n",
    "\n",
    "\n",
    "def generate_text_with_attn_positions(model, tokenizer, start_text, max_length, device, eot_string=\"<|endoftext|>\", temperature=1, topk=1, output_only=False):\n",
    "    \"\"\"\n",
    "    Ensure tokenizer decode is set to tokenizers.decoders.WordPiece() for best results\n",
    "    max_length refers to number of tokens\n",
    "    device can be either \"cpu\" or \"cuda\"\n",
    "    topk must be >= 1\n",
    "    \"\"\"\n",
    "    eot_token = tokenizer.encode(eot_string).ids[0]\n",
    "    start_tokens = tokenizer.encode(start_text).ids\n",
    "    start_tokens_len = len(start_tokens)\n",
    "    start_tokens = torch.tensor(start_tokens).to(device)\n",
    "    tokens = generate_tokens_with_attn_positions(model, start_tokens, max_length, device, eot_token, temperature=temperature, topk=topk)\n",
    "\n",
    "    if output_only:\n",
    "        return tokenizer.decode(tokens[0][start_tokens_len:].tolist(), skip_special_tokens=False)\n",
    "    return tokenizer.decode(tokens[0].tolist(), skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd55ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a16934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    start_text, \n",
    "    max_beams=5, \n",
    "    max_tokens=512,\n",
    "    eot_token=\"<|endoftext|>\"\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    eot_token_id = tokenizer.encode(eot_token).ids[0]\n",
    "    start_tokens = torch.tensor(tokenizer.encode(start_text).ids, device=device)\n",
    "\n",
    "    # Initialize beams\n",
    "    beams = start_tokens.unsqueeze(0).repeat(max_beams, 1)\n",
    "    beam_scores = torch.zeros(max_beams, device=device)\n",
    "    beam_scores[0] = 1.0  # Start with one active beam\n",
    "    completed_beams = []\n",
    "\n",
    "    for step in range(max_tokens - start_tokens.size(0)):\n",
    "        num_tokens = beams.size(1)\n",
    "        full_attn_mask = torch.ones((max_beams, num_tokens, num_tokens), device=device)\n",
    "        positions = torch.arange(num_tokens, device=device).unsqueeze(0).expand_as(beams)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = model([beams, full_attn_mask, positions])[:, -1, :]\n",
    "            log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Get top candidates\n",
    "        adjusted_scores = beam_scores.unsqueeze(1) + log_probs\n",
    "        top_scores, top_indices = adjusted_scores.view(-1).topk(max_beams)\n",
    "        beam_indices = top_indices // log_probs.size(-1)\n",
    "        token_indices = top_indices % log_probs.size(-1)\n",
    "\n",
    "        # Update beams and histories\n",
    "        new_beams = torch.cat([beams[beam_indices], token_indices.unsqueeze(1)], dim=1)\n",
    "\n",
    "        beams, beam_scores = new_beams, top_scores\n",
    "\n",
    "        # Check for completion\n",
    "        completed = token_indices == eot_token_id\n",
    "        if completed.any():\n",
    "            for i in range(max_beams):\n",
    "                if completed[i]:\n",
    "                    completed_beams.append(beams[i].cpu())\n",
    "                    beam_scores[i] = float(\"-inf\")  # Disable completed beams\n",
    "\n",
    "        if len(completed_beams) >= max_beams:\n",
    "            break\n",
    "\n",
    "    return [tokenizer.decode(beam.tolist()[:-1], skip_special_tokens=False) for beam in completed_beams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf386d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce83041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    top_p=0.95,\n",
    "    temperature=1.0,\n",
    "    max_length=512,\n",
    "    eot_token=\"<|endoftext|>\"\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    eot_token_id = tokenizer.encode(eot_token).ids[0]\n",
    "\n",
    "    prompt_tokens = tokenizer.encode(prompt).ids\n",
    "    input_ids = torch.tensor(prompt_tokens, device=device).unsqueeze(0)\n",
    "\n",
    "    for step in range(max_length - len(prompt_tokens)):\n",
    "        num_tokens = input_ids.size(1)\n",
    "        full_attn_mask = torch.ones((1, num_tokens, num_tokens), device=device)\n",
    "        positions = torch.arange(num_tokens, device=device).unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = model([input_ids, full_attn_mask, positions])[:, -1, :]\n",
    "\n",
    "        logits = logits / temperature\n",
    "        logits = logits.squeeze(0)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        keep_mask = cumulative_probs <= top_p\n",
    "        keep_mask[..., 0] = 1\n",
    "        filtered_probs = sorted_probs * keep_mask.float()\n",
    "        filtered_probs /= filtered_probs.sum(dim=-1, keepdim=True)\n",
    "        sampled_idx = torch.multinomial(filtered_probs, num_samples=1)\n",
    "        sampled_token = sorted_indices.gather(-1, sampled_idx)\n",
    "        sampled_token = sampled_token.unsqueeze(0)\n",
    "        # print(\"input ids shape\", input_ids.shape)\n",
    "        # print(\"sampled token shape\", sampled_token.shape)\n",
    "        input_ids = torch.cat([input_ids, sampled_token], dim=-1)\n",
    "        if sampled_token.item() == eot_token_id:\n",
    "            break\n",
    "    \n",
    "    output_tokens = input_ids.squeeze(0).tolist()\n",
    "    output_text = tokenizer.decode(output_tokens[:-1], skip_special_tokens=False)\n",
    "    return output_text\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12775dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8501f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
