{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97794ab5-552b-46ca-9605-def064bff63f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9dcd434-5cb8-4129-90ed-04dcce22330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"}) \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1fc9c17-b806-44ba-a5ec-f0a248a2e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee1aba-4e64-4770-8b7f-f4d8ed24d5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3960b0-3759-47d2-84f4-bf1263acac16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4789d28-fc9a-48c5-b4d3-b7d46e67e925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV2(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt).ids\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e9d662-9987-419a-962e-b4de967170c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_dataloader_v2(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")\n",
    "    dataset = GPTDatasetV2(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb069230-c7f5-4a91-90b6-80982c473bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ef233-3fd5-47a6-8368-98f1c631906f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb800b7-a2e3-48d7-b987-71866f1010a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# implements splitting text based on endoftext token\n",
    "# possible improvements:\n",
    "#  sequence packing + accompanying mask (start by geting distribution of training item lengths)\n",
    "#  implement sliding window chunking for text longer than max_length\n",
    "class GPTDatasetV3(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride, split_on=\"<|endoftext|>\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.endoftext_id = tokenizer.encode(split_on).ids[0]\n",
    "\n",
    "        self.text_split = txt.split(split_on)\n",
    "            \n",
    "\n",
    "    def _pad_tokens_torch(self, tokens, max_length, pad_token_id):\n",
    "        tensor = torch.full((max_length,), pad_token_id, dtype=torch.long)\n",
    "        tokens = torch.tensor(tokens[:max_length])\n",
    "        tensor[:len(tokens)] = tokens\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_split)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # max_length = min(self.max_length, self.max_tokens_length)\n",
    "        text = self.text_split[idx]\n",
    "        tokens = self.tokenizer.encode(text).ids\n",
    "        tokens_padded = self._pad_tokens_torch(tokens, self.max_length+1, self.endoftext_id)\n",
    "        return tokens_padded[:self.max_length], tokens_padded[1:self.max_length + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919aa2d8-d972-4edd-9ee2-2049960e5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "def create_dataloader_v3(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")\n",
    "    dataset = GPTDatasetV3(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=4 if num_workers > 0 else None\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694d667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
