{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a694d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load your tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"./TinyStories_tokenizer.json\")\n",
    "\n",
    "# Enable truncation (but not padding - we'll handle that in packing)\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "endoftext_token = tokenizer.encode(\"<|endoftext|>\").ids  # This is the end of text token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af80deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "def pack_sequences(examples, max_length=512, pad_token_id=endoftext_token[0]):\n",
    "    # Flatten all input_ids and attention_masks\n",
    "    input_ids = list(chain(*examples[\"input_ids\"]))\n",
    "    attention_mask = list(chain(*examples[\"attention_mask\"]))\n",
    "    \n",
    "    # Calculate number of chunks with stride = max_length\n",
    "    # Each sequence will be max_length+1 tokens long (input + prediction target)\n",
    "    num_chunks = (len(input_ids) - 1) // max_length  # We need at least 1 extra token\n",
    "    \n",
    "    # Initialize containers for packed sequences\n",
    "    packed_input_ids = []\n",
    "    packed_attention_mask = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_pos = i * max_length\n",
    "        end_pos = start_pos + max_length + 1  # +1 for prediction target\n",
    "        \n",
    "        # Extract the sequence\n",
    "        chunk_input_ids = input_ids[start_pos:end_pos]\n",
    "        chunk_attention_mask = attention_mask[start_pos:end_pos]\n",
    "        \n",
    "        # Pad if necessary (only for the last chunk)\n",
    "        if len(chunk_input_ids) < max_length + 1:\n",
    "            pad_len = (max_length + 1) - len(chunk_input_ids)\n",
    "            chunk_input_ids.extend([pad_token_id] * pad_len)\n",
    "            chunk_attention_mask.extend([0] * pad_len)\n",
    "        \n",
    "        packed_input_ids.append(chunk_input_ids)\n",
    "        packed_attention_mask.append(chunk_attention_mask)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    packed_input_ids = np.array(packed_input_ids)\n",
    "    packed_attention_mask = np.array(packed_attention_mask)\n",
    "    \n",
    "    # Split into inputs and targets\n",
    "    return {\n",
    "        \"input_ids\": packed_input_ids[:, :-1],  # All tokens except last\n",
    "        \"attention_mask\": packed_attention_mask[:, :-1],\n",
    "        \"labels\": packed_input_ids[:, 1:],  # All tokens except first (shifted by 1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aad9c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15600057/15600057 [05:41<00:00, 45656.77 examples/s]\n",
      "Map: 100%|██████████| 157832/157832 [00:03<00:00, 47356.61 examples/s]\n",
      "Saving the dataset (14/14 shards): 100%|██████████| 1022039/1022039 [00:14<00:00, 68402.30 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10326/10326 [00:00<00:00, 117611.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": \"../data/TinyStories/TinyStoriesV2-GPT4-train.txt\", \"valid\": \"../data/TinyStories/TinyStoriesV2-GPT4-valid.txt\"})\n",
    "\n",
    "# First tokenize without padding\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the batch\n",
    "    encodings = tokenizer.encode_batch_fast(examples[\"text\"])\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    return {\n",
    "        \"input_ids\": [encoding.ids for encoding in encodings],\n",
    "        \"attention_mask\": [encoding.attention_mask for encoding in encodings],\n",
    "    }\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Shuffle before packing to get better packing efficiency\n",
    "shuffled = tokenized.shuffle(seed=42)\n",
    "\n",
    "# Then apply packing\n",
    "packed_dataset = shuffled.map(\n",
    "    lambda x: pack_sequences(x),\n",
    "    batched=True,\n",
    "    batch_size=1000  # Adjust based on your memory\n",
    ")\n",
    "\n",
    "# Convert to PyTorch format\n",
    "packed_dataset.set_format(\"torch\")\n",
    "\n",
    "packed_dataset.save_to_disk(\"packed_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "120e2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "\n",
    "from datasets import load_from_disk\n",
    "packed_dataset2 = load_from_disk(\"packed_dataset\")\n",
    "packed_dataset2.set_format('torch')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "# Create DataLoader\n",
    "dataloader_train = DataLoader(packed_dataset2[\"train\"], batch_size=8, shuffle=True)\n",
    "dataloader_valid = DataLoader(packed_dataset2[\"valid\"], batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6d7bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first batch\n",
    "for batch in dataloader_train:\n",
    "    first_batch = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d018431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  16,    6, 4113,  ...,   18, 4145,  256],\n",
       "         [ 375,  932,  367,  ...,  231,  550,  324],\n",
       "         [ 987,  326,   68,  ...,  277,  225,  408],\n",
       "         ...,\n",
       "         [ 269,  289, 1652,  ...,  238,   68,  334],\n",
       "         [ 354,  579,  890,  ...,  417,  604,  279],\n",
       "         [ 258,  429,  289,  ..., 4852,  228, 1381]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[   6, 4113,  227,  ..., 4145,  256,  407],\n",
       "         [ 932,  367,  256,  ...,  550,  324,  566],\n",
       "         [ 326,   68,  879,  ...,  225,  408,   16],\n",
       "         ...,\n",
       "         [ 289, 1652,  227,  ...,   68,  334,  394],\n",
       "         [ 579,  890,  707,  ...,  604,  279,  517],\n",
       "         [ 429,  289, 5088,  ...,  228, 1381,  277]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab69043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
